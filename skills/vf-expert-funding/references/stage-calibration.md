# Stage Calibration Guide

What "good enough" looks like for pre-product, pre-revenue ventures. This guide prevents the expert review from holding early-stage ventures to post-revenue standards.

## Core Principle

Pre-product standards are **explicitly lower** across all evaluator types. This is not inference — it is published policy:

- **NIH:** "Preliminary data are not required" for SBIR Phase I
- **Y Combinator:** "We fund companies with no revenue, product, or fully baked idea"
- **Techstars:** "We've accelerated teams that came in with zero revenue"
- **a16z:** "We don't expect a fully-baked model" at early stage
- **NSF:** Evaluates "potential" business model, not proven model

Source: NIH SBIR reviewer guide (2025), YC published FAQ, Techstars blog (2024), a16z data room guide (Moore, 2022), NSF Seed Fund criteria.

## Phase 01 — Capital Thesis Calibration

### What's Expected at Pre-Product

| Element | Expected | NOT Expected |
|---------|----------|-------------|
| Funder identification | 2+ viable sources with alignment rationale | 40+ deeply researched profiles with relationship maps |
| Criteria documentation | Primary source criteria for top targets | Revealed-vs-stated preference analysis per funder |
| Alignment path | Honest gap assessment with general plan | Specific milestone-gated funder engagement calendar |
| Timeline | Major deadlines and general process awareness | Detailed calendar with relationship-building sequences |
| Financial projections | Not expected at this phase | 3-5 year models, quarterly projections |

### Over-Engineering Signals (Phase 01)
- Detailed financial models before problem validation
- Complex funder relationship CRM before first outreach
- Extensive legal/compliance preparation before funder interest confirmed
- Multi-scenario funding strategy (optimistic/base/pessimistic) at idea stage

### Under-Development Signals (Phase 01)
- No funders identified beyond "we'll figure it out"
- Funder list copied from a database with no filtering
- No awareness of application deadlines or funding cycles
- Assumes all funders have identical evaluation criteria

## Phase 08 — Materials Assembly Calibration

### What's Expected at Pre-Product

| Element | Expected | NOT Expected |
|---------|----------|-------------|
| Market sizing | Bottom-up approach with stated assumptions | Purchased market research reports; precise TAM to the dollar |
| Traction evidence | LOIs, waitlist, customer interviews, technical milestones | Revenue data, retention cohorts, unit economics from real usage |
| Pitch deck | 10-15 slides covering essentials | Polished design-agency deck; product demo video |
| Financials | Use of funds + milestones; basic burn/runway | Quarterly projections years 1-5; detailed P&L |
| Competition | 3-5 alternatives with honest positioning | Exhaustive competitive landscape with feature matrices |
| Data room | Deck + cap table + basic financials + team + customer evidence | Tax returns, audits, board minutes, org charts |

### Over-Engineering Signals (Phase 08)
- 40+ tab Excel financial model
- Revenue projections segmented by product, region, AND customer segment
- Detailed product roadmap beyond 12-18 months
- Exit strategy slide in pre-seed deck
- Complex cap table modeling with multiple scenario analyses
- Professional design-agency pitch deck at idea stage (SVB: "a red flag")

### Under-Development Signals (Phase 08)
- No financials at all (0% of successful decks omit them — DocSend)
- Business model absent or vague
- No competition analysis
- Traction section says "strong interest" with no specifics
- Inconsistent numbers across deck, summary, and one-pager

### Acceptable Shortcuts at Pre-Product
- Mockups instead of product demos
- Market size based on founder insight rather than purchased research
- Competition limited to 3-5 alternatives
- Financial projections at annual granularity only
- No exit strategy slide (described as "amateurish" at pre-seed)
- Single use-of-funds breakdown (not detailed line items)

## Phase 12 — Funding Execution Calibration

### What's Expected at Pre-Product

| Element | Expected | NOT Expected |
|---------|----------|-------------|
| Applications | Tailored to funder type (grant vs. equity vs. accelerator) | Identical application to all funders |
| Traction evidence | Stage-appropriate: LOIs, waitlist, interviews, pilot agreements | Revenue, retention, CAC/LTV from real operations |
| Data room | 5-7 essential documents | Extensive legal, tax, audit documentation |
| Funding status | Pipeline tracking by funder type | CRM-level relationship management tracking |

### Stage-Appropriate Traction Substitutes

When no product or revenue exists, these substitutes are accepted by evaluators:

| Substitute | Evaluator Acceptance | Quality Marker |
|-----------|---------------------|----------------|
| Letters of Intent | High (angels, VCs, grants) | Specific terms, named organizations |
| Waitlist signups | Medium-High | Conversion metrics, engagement data |
| Customer interviews | Medium | Documented with quotes, pain point specificity |
| Technical milestones | High (grants especially) | Prototype stages, TRL level |
| Design partner commitments | Medium | Named partners with defined scope |
| Advisor commitments | Medium | Relevant domain expertise; active involvement |
| Accelerator acceptance | High | Serves as credibility signal |

Source: Mercury (2024), YC guidance, SBIR Phase I criteria.

### Funder-Type Expectation Differences

| Dimension | Grant Reviewers (SBIR) | Equity Investors (Angels/VCs) | Accelerators |
|-----------|----------------------|------------------------------|-------------|
| Primary focus | Technical feasibility | Team quality + coachability | Founder achievement magnitude |
| Market validation | Can be theoretical | Prefer customer evidence | Problem understanding |
| Financial projections | Not required Phase I | Simple model expected | Not typically required |
| IP status | Important for Phase II | Less critical pre-seed | Not typically required |
| Commercialization | Pathway, not proof | Business model clarity | Growth potential |
| Evaluation style | Scored rubrics (1-9) | Holistic, intuitive | Multi-round judging |

Source: NIH reviewer guide, Gompers (2020), Van Osnabrugge (2000), YC/Techstars published criteria.

## Calibration Decision Rules

When evaluating outputs, apply these rules:

1. **If the venture is pre-product:** Do not penalize for missing revenue data, retention cohorts, or detailed unit economics. Score based on quality of thinking, not comprehensiveness of proof.

2. **If the venture is pre-revenue:** Accept LOIs, waitlist, interviews, and technical milestones as traction. Penalize vague claims ("strong interest") but not absence of revenue.

3. **If targeting grants (SBIR Phase I):** Do not require preliminary data. Focus on technical approach quality and commercialization pathway plausibility.

4. **If targeting equity (angels/VCs):** Weight team credentials and founder-market fit heavily. Accept simple financial models. Penalize over-engineering more than under-development.

5. **If targeting accelerators:** Weight founder achievement magnitude. Accept idea-stage applications (YC: 40% of accepted companies are "just an idea"). Penalize inability to articulate in one sentence.

6. **Always flag over-engineering.** At pre-product stage, excessive detail signals misplaced priorities. Simplicity signals focus. This is counterintuitive but well-documented.

Source: a16z (Moore, 2022), SVB guidance, Highline Beta, First Round PMF Method Level 1 criteria.
