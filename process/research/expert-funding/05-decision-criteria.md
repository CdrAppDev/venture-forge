# Expert evaluation: what funders actually weight versus what they claim

**Funders systematically misreport their own decision criteria.** Academic research using field experiments, internal VC memos, and longitudinal outcome tracking reveals a consistent pattern: evaluators across venture capital, grant review, and accelerator selection state that team quality matters most, but their actual behavior shows business fundamentals, market size, and prior track record drive decisions. VCs replace founders in **71-87%** of successful companies while business models remain stable in **98%** of cases—the opposite of what "we invest in great teams" rhetoric suggests.

This gap between stated and revealed preferences has profound implications for founders and researchers crafting materials for evaluation. The signals that capture initial attention differ from those that ultimately drive investment decisions, and evaluators spend far less time reviewing materials than commonly believed—**under 2 minutes** on average for seed-stage pitch decks in 2023, down from nearly 4 minutes in 2015.

---

## The team-versus-business paradox in VC decisions

The largest and most rigorous study of venture capital decision-making surveyed **885 institutional VCs at 681 firms**. When asked directly, **95%** cited management team as important and **47%** named it their single most important criterion. Yet behavioral evidence tells a different story.

Kaplan, Sensoy, and Strömberg tracked **50 VC-financed companies** from initial business plan through IPO and beyond. Their findings upend conventional wisdom: only **29%** of founders remained as CEO at IPO, and just **13%** three years post-IPO. Meanwhile, business lines remained "remarkably stable"—only **1 of 50 companies** significantly changed their core business model. The researchers concluded that "investors in start-ups should place more weight on the business ('the horse') than on the management team ('the jockey')."

A randomized field experiment on AngelList provides crucial nuance. Bernstein, Korteweg, and Laws sent **16,981 emails** to **4,494 active investors** with randomly varied information about team credentials, existing investors, and traction. Team information significantly increased click-through rates, while traction and existing investor signals showed no significant effect. This reconciles the apparent contradiction: **team quality functions as a screening filter** that captures attention, but business fundamentals determine outcomes.

| Methodology | What Predicts Success | Source |
|-------------|----------------------|--------|
| Survey (stated) | Team > Business | Gompers et al., 2020 |
| Field experiment (attention) | Team credentials | Bernstein et al., 2017 |
| Longitudinal outcomes | Business stability > Team stability | Kaplan et al., 2009 |
| Internal VC memos | Market, management, strategy weighted ~equally | Kaplan & Strömberg, 2004 |
| Real-time conjoint | Revenue growth > Product > Team track record | Block et al., 2019 |

Analysis of **67 actual VC investment memoranda** reveals what VCs document internally when making decisions. Market size and growth were cited as a strength in **69%** of deals, management quality in **60%**, and business strategy in **54%**. Critically, **51%** of investments included an explicit expectation that the VC would recruit or change management—documented proof that "betting on the team" often means betting on the VC's ability to upgrade that team.

**Source:** Gompers, Gornall, Kaplan & Strebulaev, "How Do Venture Capitalists Make Decisions?" Journal of Financial Economics, 2020. URL: https://www.nber.org/papers/w22587 | **Relevance:** Largest VC survey ever conducted with 885 respondents.

**Source:** Kaplan, Sensoy & Strömberg, "Should Investors Bet on the Jockey or the Horse?" Journal of Finance, 2009. URL: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=657721 | **Relevance:** Longitudinal tracking reveals actual founder replacement rates.

**Source:** Bernstein, Korteweg & Laws, "Attracting Early-Stage Investors: Evidence from a Randomized Field Experiment," Journal of Finance, 2017. URL: https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2432044 | **Relevance:** Only randomized field experiment on investor attention.

---

## Grant reviewers weight reputation despite official criteria emphasizing proposed work

Federal grant review processes publish detailed scoring rubrics, yet empirical research consistently shows reviewers systematically deviate from these frameworks. The NIH's traditional five criteria—Significance, Investigator, Innovation, Approach, and Environment—were designed to evaluate proposed work on its merits. In practice, reviewer behavior suggests applicant reputation dominates.

A qualitative study of **65 French grant reviewers** found that when assessing feasibility, reviewers overwhelmingly relied on **applicant reputation and prior publications** rather than evaluating the proposed methodology. One reviewer stated explicitly: "What matters regarding feasibility is the applicant's reputation, not practical feasibility." Assessment checklists were typically completed **at the end of review** to support pre-formed opinions rather than to develop them.

Analysis of **42,905 scored reviews** for Hungarian research grants found that **past scientometric performance of the principal investigator is the best predictor of future output**—better than reviewer scores. The correlation between PI's prior H-index and research output was **0.45-0.54**, while correlation between reviewer scores and Q1 publications was only **0.08-0.11**.

Inter-rater reliability in grant review is disturbingly low. A meta-analysis of **48 studies** covering **19,443 manuscripts** found a mean inter-rater correlation of just **0.34**. For Norwegian research funders analyzing **134,991 reviews**, grant reviews "almost never reach what is considered acceptable reliability" (ICC of 0.75 threshold). This suggests significant randomness in what gets funded.

The NIH explicitly acknowledged reviewer bias in its **January 2025 Simplified Review Framework**, which converted Investigator and Environment assessments from scored criteria (1-9 scale) to **binary evaluation** (sufficient/not sufficient). The stated rationale addressed "the potential for reputational bias to affect peer review outcomes."

**Triage eliminates roughly half of applications** before discussion. At NIH, approximately **50%** of applications are "streamlined"—receiving written critiques but no overall impact score and no panel discussion. Non-streamlined applications receive approximately **15-20 minutes** of discussion time. A 1993 NIH validation study found the probability of eliminating a highly competitive application through triage was ≤0.014, but this assumes reviewer quality assessments are accurate.

**Source:** Abdoul et al., "Peer Review of Grant Applications: Criteria Used and Qualitative Study of Reviewer Practices," PLOS ONE, 2012. URL: PMC3460995 | **Relevance:** Only in-depth qualitative study of grant reviewer behavior (n=65).

**Source:** NIH Center for Scientific Review, Simplified Review Framework. URL: https://grants.nih.gov/policy-and-compliance/policy-topics/peer-review/simplifying-review/framework | **Relevance:** Official acknowledgment of reviewer bias driving policy change.

---

## Investors spend under two minutes reviewing pitch decks

DocSend's tracking data, collected from their document-sharing platform, reveals a dramatic decline in investor attention. Average viewing time for seed-stage pitch decks fell from **3 minutes 44 seconds** in 2015 to **under 2 minutes** in 2023—a 48% decrease. This means investors are making initial screening decisions based on roughly **10 seconds per slide** in a 12-slide deck.

The sections receiving most scrutiny are **financials, team, and competition**, in that order. A 2015 collaboration between DocSend and Harvard Business School professor Tom Eisenmann analyzed **200+ companies** that raised $360 million total. More recent data shows a shift: in 2022-2023, time spent on **business model** increased 48%, **financials** increased 48%, and **traction** increased 25%. Investors appear to be front-loading their assessment of whether unit economics work.

Deck structure correlates with outcomes. **Successful decks place Product and Business Model sections early**, while unsuccessful decks bury these in the middle. The company introduction slide has become a critical "gatekeeper"—investors use it to decide whether to continue reading. DocSend found this section's importance "really shot up" in recent years.

The funnel is brutal: VCs receive approximately **200 deals per year**, review roughly **100**, pursue due diligence on **23**, and invest in **4**. This implies a **99% rejection rate** at the screening phase. From first contact to deal close takes an average of **83 days** (median 62 days), and founders need approximately **40 investor meetings** to close a round.

**Source:** DocSend/Harvard Business School Pitch Deck Study, 2015. URL: https://techcrunch.com/2015/06/08/lessons-from-a-study-of-perfect-pitch-decks-vcs-spend-an-average-of-3-minutes-44-seconds-on-them/ | **Relevance:** Original benchmark study with behavioral tracking data.

**Source:** DocSend Annual Startup Fundraising Reports, 2021-2024. URL: https://www.docsend.com/startup-fundraising/ | **Relevance:** Longitudinal tracking showing attention decline.

---

## Quick-kill factors that eliminate ventures before full review

Evaluators employ consistent heuristics to rapidly filter opportunities. These "quick kills" operate before detailed analysis begins.

**Investment thesis misalignment** eliminates deals immediately: wrong stage (seed fund seeing Series B deal), wrong geography (outside fund's investment area), wrong sector (healthcare fund seeing consumer app). Gompers et al. note that "a large percentage of investment opportunities can be eliminated from consideration in minutes due to screening processes in place."

For grant applications, the **triage process eliminates roughly 50%** of submissions without full panel discussion. Applications with the worst preliminary scores from three assigned reviewers are nominated for non-discussion, though any single reviewer can "rescue" an application.

Y Combinator partners cite several quick-rejection factors: inability to explain the company in one sentence, solo founders (with rare exceptions), memorized answers suggesting lack of genuine expertise, future-focused responses without current reality, and poor metrics knowledge. Techstars emphasizes **coachability** as a critical screen—founders who appear resistant to feedback are eliminated early.

Accelerator research reveals a documented gap between stated stage-agnosticism and actual behavior. Academic meta-analysis found that "many accelerator programs tend to favor start-ups that have already obtained some form of funding before entering the program"—despite claims of accepting pre-product companies.

**What signals override detailed analysis:**

- **Team credentials** capture initial attention and pass screening gates
- **Traction metrics** (revenue growth, user acquisition) increasingly dominate later evaluation
- **Social proof** from existing investors or network referrals
- **Elite school attendance, founder demographics, and geographic region** affect valuations in ways not captured by official criteria

**Source:** Y Combinator, How to Apply. URL: https://www.ycombinator.com/howtoapply | **Relevance:** Official criteria from highest-profile accelerator.

**Source:** Butz & Mrożewski, "The Selection Process and Criteria of Impact Accelerators," MDPI Sustainability, 2021. URL: https://www.mdpi.com/2071-1050/13/12/6617 | **Relevance:** Systematic comparison of stated vs. actual accelerator criteria.

---

## What makes evaluators lean in versus pass

Analysis across evaluator types reveals consistent patterns in what triggers deeper engagement versus rejection.

**In pitch materials**, successful decks demonstrate three qualities: contrarian insight (something the founder understands that others miss), evidence of early customer love (not just usage metrics, but engaged, passionate users), and clarity on market dynamics and competitive positioning. First Round Capital explicitly asks: "What do you understand about a market or need that no one else does?"

**In grant applications**, the factors that score high are not those officially weighted most heavily. While NIH criteria emphasize Innovation and Significance, reviewer behavior shows that **Originality** (cited by 12/34 reviewers in one study) and **Methodology** (10/34) receive priority attention. Feasibility assessment relies heavily on the applicant's prior publication record rather than the proposed approach.

**For pre-product ventures**, evaluators face maximum uncertainty. Academic research on angels shows they use **non-compensatory decision-making**—certain criteria can disqualify entirely regardless of other strengths. Trustworthiness of the entrepreneur ranked highest in angel investment criteria studies, followed by management team quality and entrepreneur enthusiasm. Exit opportunities ranked fourth despite early-stage uncertainty about outcomes.

A16z articulates their approach to uncertainty: "The nature of the bet is asymmetric. We think far more about how big the outcome will be if a deal succeeds than all the ways it can fail." This asymmetric framing—focusing on upside magnitude rather than probability of success—characterizes sophisticated early-stage evaluation.

**How evaluators handle missing data:** At early stages, evaluators substitute proxies. Without revenue, they look for user engagement metrics. Without users, they assess founder domain expertise and prior execution. Without a complete team, they evaluate the lead founder's ability to recruit. First Round's PMF Method provides explicit guidance: at the earliest stages, emphasize **Satisfaction** (customer engagement) over **Efficiency** (unit economics), accepting that efficiency data won't exist yet.

**Source:** a16z Investment Philosophy. URL: https://a16z.com/about/ | **Relevance:** Major VC's stated approach to asymmetric bets.

**Source:** Sudek, "Angel Investment Criteria," Journal of Small Business Strategy, 2007. URL: https://jsbs.scholasticahq.com/article/26530.pdf | **Relevance:** Empirical study ranking angel criteria by actual importance.

---

## The consistency gap: evaluators disagree with each other and themselves

Beyond the stated-versus-revealed gap, evaluators show poor consistency. For grant review, the mean inter-rater reliability of **0.17-0.37** means two reviewers evaluating the same proposal often reach substantially different conclusions. One analysis found that when restricting to high-quality proposals (the decisions that matter most), inter-rater reliability approaches zero.

VCs demonstrate similar inconsistency. Conjoint analysis research using **749 private equity investors** making real-time decisions found that revenue growth, product value-add, and team track record were weighted differently across investor types. VCs prioritized revenue growth and business models; family offices prioritized profitability. This variation means the same venture may receive radically different evaluations depending on which investor reviews it.

**Reviewers are aware of these problems.** Content analysis of **270 experienced peer reviewers** found the most frequent comments about peer review were negative. Reviewers cited "apathy, not taking the process seriously, nit-picking, tribalism, rigidity, negativity, and the formation of reviewer gangs." One NIH reviewer stated: "I think in general NIH disfavors innovation. Even sitting on R21 panels, risky projects are often torpedoed."

The "Matthew Effect" compounds these issues. Analysis of scientists just above versus just below funding thresholds found that those who received initial funding accumulated **more than twice as much research funding** over the following eight years. Near-miss scientists applied for grants less often after initial failure, suggesting early randomness in evaluation creates persistent divergence in outcomes.

**Source:** Bornmann et al., "A Meta-Analysis of Inter-Rater Reliability," PLOS ONE, 2010. | **Relevance:** Definitive meta-analysis on reviewer disagreement (48 studies, 19,443 manuscripts).

**Source:** Block, Fisch, Vismara & Andres, "Private Equity Investment Criteria: An Experimental Conjoint Analysis," Journal of Corporate Finance, 2019. URL: https://www.sciencedirect.com/science/article/pii/S0929119918307132 | **Relevance:** Real-time decision experiment with 749 investors.

---

## Mapping criteria to specific outputs

| Material Element | What Makes Evaluators Lean In | What Triggers Pass/Rejection |
|------------------|------------------------------|------------------------------|
| **Executive summary / company intro** | Clear, contrarian insight; specific problem definition | Jargon-heavy; generic market description |
| **Team section** | Domain expertise; prior execution evidence; complementary skills | Solo founder; no technical cofounder; team gaps |
| **Market sizing** | Bottom-up analysis; specific customer segments | Top-down TAM only; "huge market" claims without specificity |
| **Traction metrics** | Week-over-week growth; customer engagement depth | Vanity metrics; inconsistent data |
| **Financials** | Clear unit economics; capital efficiency | Missing projections (31% of early-stage VCs don't require these); unrealistic assumptions |
| **Competition slide** | Honest assessment; differentiated positioning | Claiming "no competition"; misunderstanding market |
| **Ask / use of funds** | Specific milestones; stage-appropriate amount | Vague deployment; misaligned with fund size |

For grant applications, the mapping differs:

| Application Element | High Scores | Triage Risk |
|---------------------|-------------|-------------|
| **Specific Aims** | Clear hypothesis; measurable outcomes; feasibility | Overly ambitious scope; unclear endpoints |
| **Significance** | Novel contribution; field advancement | Incremental work; "more of same" |
| **Innovation** | New methods or approaches | Standard methodology without justification |
| **Approach** | Rigorous methodology; adequate sample sizes; alternative strategies | Methodological gaps; no contingency plans |
| **Investigator** | Track record of productivity (despite this officially being de-emphasized) | Unproven in the specific domain |

---

## Conclusion

The research consensus reveals a fundamental paradox in venture evaluation: **evaluators consistently articulate criteria they do not reliably apply**. VCs claim team primacy while replacing most founders; grant reviewers follow official rubrics while actually weighting applicant reputation; accelerators profess stage-agnosticism while favoring already-funded startups.

For founders and researchers, the practical implications are clear. **Team quality serves as a screening filter**—necessary to pass initial gates but not sufficient for ultimate success. What drives final decisions is evidence of **business fundamentals**: market size, revenue trajectory, customer engagement, and execution track record.

Materials should be optimized for the actual evaluation process: **under 2 minutes** of investor attention, **50% triage rates** in grant review, and **non-linear reading patterns** where evaluators jump to sections of interest. Front-load the company introduction and financials rather than burying them. Demonstrate contrarian insight early. Accept that inter-rater reliability is low and persistence across multiple reviewers may matter as much as optimizing for any single evaluator's stated preferences.

The gap between stated and revealed preferences is not hypocrisy—it reflects the difficulty of articulating intuitive pattern-matching. But understanding what evaluators actually do, rather than what they claim, provides significant advantage in crafting materials that survive the funnel.