# Early-Stage Venture Evaluation Frameworks: A Comprehensive Reference

**Team quality dominates all evaluation frameworks**—whether federal grants, venture capital, or accelerators—but the specific scoring mechanisms and knockout criteria vary dramatically across evaluator types. This research synthesizes 30+ official frameworks revealing that structured rubrics exist primarily in government programs, while private investors rely on heuristic-based frameworks with implicit weightings. The most actionable scoring systems come from NIH (1-9 numerical scale), First Round Capital (4-level PMF framework with quantitative benchmarks), and MassChallenge (transparent multi-round judging).

---

## Federal grant programs offer the most explicit scoring rubrics

Government grant programs—particularly SBIR/STTR across agencies—provide the most transparent and replicable evaluation frameworks. Unlike private investors who rarely publish scoring systems, federal agencies are required to document merit review criteria.

### NIH uses the only true numerical scoring system (1-9 scale)

The National Institutes of Health peer review system provides the most granular scoring methodology across all evaluation sources researched. Reviewers assign criterion scores on a **1-9 scale** for five dimensions:

| Score | Descriptor | Guidance |
|-------|------------|----------|
| 1 | Exceptional | Highest quality, no weaknesses |
| 2-3 | Outstanding/Excellent | Extremely strong, negligible to minor weaknesses |
| 4-5 | Very Good/Good | Strong with moderate to important weaknesses |
| 6-7 | Satisfactory/Fair | Some strengths but moderate to major weaknesses |
| 8-9 | Marginal/Poor | Few strengths, numerous major weaknesses |

**Five core criteria evaluated:** Significance (including commercial potential), Investigator qualifications, Innovation, Approach (methodology and feasibility), and Environment (resources and facilities). The **Overall Impact Score** is NOT a mathematical average—it represents a "gestalt" evaluation of likelihood the project will have sustained, powerful influence. Final scores are calculated as mean of all impact scores × 10, yielding a **10-90 scale** where scores of approximately **10-30 are typically competitive** for funding.

Phase II applications require a **Commercialization Plan** addressing: value proposition, company information, market/customer/competition analysis, IP protections, finance plan, production/marketing plan, and revenue generation strategy. Applications scoring in the bottom ~50% are "triaged" (not discussed).

### NSF and DoD use criteria-based merit review without numerical scores

NSF SBIR/STTR evaluates three criteria—Intellectual Merit, Broader Impacts, and Commercial Impact—without assigning numerical scores. A minimum of three expert reviewers provide written critiques, and Program Directors make funding determinations based on reviews plus field knowledge. **Historical funding rate: 10-20%** of Phase I proposals.

The Department of Defense lists three criteria in **descending order of importance**: Technical Merit (most important), Key Personnel Qualifications (second, equal to third), and Commercial Potential. No numerical scoring scale is specified in the general BAA, though component-specific instructions may add sub-dimensions with ratings like "Ineffective," "Adequate," "Effective," "Exceptional."

### DOE provides clear weighting: three criteria equally weighted at 33% each

The Department of Energy offers explicit weighting for Phase I and initial Phase II: Strength of Scientific/Technical Approach (33%), Ability to Carry Out the Project (33%), and Impact of the Proposed Project (33%). For Phase IIB and IIC, Impact has **twice the weight** of other criteria.

**Critical threshold:** Applications are considered for funding only if they receive the **highest rating in at least 2 of 3 criteria**. Even "excellent" proposals may not be funded if topics are oversubscribed.

### Universal federal knockout criteria

Across all agencies, administrative requirements serve as hard knockout criteria:
- Failure to meet small business eligibility (≤500 employees)
- Principal investigator not primarily employed by small business
- Non-compliance with proposal format requirements
- Missing required certifications and registrations
- For experienced firms: failure to meet performance benchmarks (Phase I to Phase II transition rate, commercialization evidence)

---

## VC frameworks emphasize "magnitude of strength" over systematic scoring

Unlike federal programs, venture capital firms rarely publish formal scoring rubrics. However, several firms have documented structured evaluation frameworks.

### First Round Capital's PMF Method: the most actionable VC framework

First Round Capital's "4 Levels, 3 Dimensions, 4 Levers" framework published in April 2024 provides the most structured VC evaluation system with specific quantitative benchmarks at each stage:

**The four levels of product-market fit with benchmarks:**

| Level | Stage | ARR Range | Key Metrics |
|-------|-------|-----------|-------------|
| **Level 1: Nascent** | Pre-seed/Seed | $0-$500K | 3-5 customers via warm intros; 1/10-20 intro conversion |
| **Level 2: Developing** | Seed/Series A | $500K-$5M | 10% first call to closed-won; NRR ≥100%; churn 10-20%; Gross margin ≥50%; Burn multiple ≤5X |
| **Level 3: Strong** | Series A/B/C | $5M-$25M | Magic Number >0.75; CAC payback <18 months; NRR >110%; churn <10%; Gross margin 60-70%+; Burn multiple 1-3X |
| **Level 4: Extreme** | Series C+ | $25M+ | >15% first call to closed-won; CAC payback <12 months; NRR >120%; Gross margin 80%+; Burn multiple 0-1X |

**Three evaluation dimensions** applied at each level: Satisfaction (retention, NPS, usage), Demand (sales velocity, conversion rates, ACV growth), and Efficiency (CAC, payback, LTV/CAC, burn multiple).

**Level 1 red flags for pre-product ventures:**
- Customers wouldn't be disappointed if product disappeared
- Usage low and not growing over 6 months
- Each customer needs different key features (consulting territory)
- Sales cycles too long; deal collapses if champion leaves
- Design partners hesitant to convert to paid

### Sequoia's two-criteria investment philosophy

Sequoia Capital's published framework distills evaluation to two core factors: **(1) a unique insight about an important problem in a big market**, and **(2) the right team to tackle it**. Their business plan framework expects 10 slides covering: Company Purpose (single declarative sentence), Problem, Solution ("explain your eureka moment"), **Why Now** (emphasized as critical), Market Potential, Competition, Business Model, Team, Financials, and Vision.

Don Valentine's market philosophy remains influential: "We're never interested in creating markets—it's too expensive. We're interested in exploiting markets early."

### a16z evaluates on "magnitude of strength"

Andreessen Horowitz's published philosophy from Ben Horowitz: **"Evaluate on magnitude of strength. Are they world-class at something? Literally the best in the world at one thing that matters? Passing because monetization is unclear or go-to-market hasn't been proven wastes time. You can always find weaknesses."**

a16z Bio + Health adds healthcare-specific criteria: teams with depth in both AI/tech AND healthcare commercialization, deep understanding of complex healthcare value chain, and products addressing top pain point of workforce retention. Partner Julie Yoo notes: "When we partner with companies at the seed stage, the vast majority of what we index on is the founder."

### Climate tech frameworks prioritize quantified impact

**Breakthrough Energy Ventures** requires potential to **"reduce greenhouse gases by at least half a gigaton every year, about 1 percent of global emissions"**—a hard quantitative threshold. They operate on a **20-year return horizon** versus typical 5-year VC timelines.

**Third Derivative** provides the most explicit climate impact thresholds:
- Direct carbon removal: Potential to avoid/remove >0.25 Gt CO2e globally or >1% regionally
- Critical enablers: >0.5 Gt globally or >2% regionally  
- Non-critical enablers: >1.0 Gt globally or >5% regionally

---

## Accelerators weight team at 50%+ with transparent selection criteria

### Y Combinator: founders are the primary criterion

Y Combinator's application process prioritizes in order: **Founders** (most important—"something impressive each founder has built or achieved"), Clarity of Communication (matter-of-fact, specific descriptions), Idea/Insight Quality (level of insight, not just idea type), and Progress/Traction.

**Critical statistics:** Partners spend ~20 seconds on initial scan. **40% of accepted companies are "just an idea"** with no revenue. Acceptance rate: **1.5-2%**. Standard investment: $500,000.

**Knockout criteria:** Vague or marketing-heavy descriptions, no clear founder achievements, inability to articulate competitive advantages, generic claims without specific evidence.

### Techstars explicitly weights team at majority importance

Techstars documentation states evaluation dimensions in order: "Team, Team, Team, Market, Product, Traction"—indicating team comprises **50%+ of decision-making weight**.

**Four specific team attributes evaluated:**
1. **Coachability** - Willingness to accept feedback
2. **Execution** - Working hard AND smart; ability to prioritize
3. **Prior History** - Previous demonstrations of effective execution
4. **"Give First" Culture** - Generosity and values alignment

Key interview question: "Why are you and your founders the best people in the world to solve this problem?"

### MassChallenge provides the most transparent judging rubric

As a non-profit taking zero equity, MassChallenge publishes detailed judging procedures:

**Primary criteria:** Potential for Impact (most important—self-defined by startup) and Path to Achieving Impact (growth potential, industry understanding, team composition).

**Multi-round process:**
- Round 1: All applications reviewed by 3+ judges who complete mandatory bias training
- Judges provide recommendation on scale from "definitely no" to "definitely yes"
- Round 2: Semi-finalists pitch to expert panel with industry-specific judge matching
- Scores aggregated; no single judge determines outcome

**Eligibility thresholds:** Less than $1M in equity-based funding raised, less than $2M in annual revenue. Top 2% advance to final judging; approximately 1/3 of semi-finalists make accelerator.

### Domain-specific programs add quantitative requirements

**Third Derivative** (climate): Requires minimum 2 full-time employees, working prototype (TRL 4+), for-profit status, and **no basic science risk**. Selection rate ~5%.

**StartX** (Stanford): At least one founder must have Stanford affiliation. Selection rate ~8%. Claims 92% survival rate over 10 years and 3x higher chance of $100M+ valuation versus other top accelerators.

**Elemental Excelerator**: Requires Seed to Series C stage, minimum 2 full-time employees, working prototype, and existing funding. Selection rate ~2%.

---

## Academic research validates team as the primary decision factor

### The Gompers study: largest empirical analysis of VC decision-making

**Gompers, Gornall, Kaplan & Strömberg (2020)** surveyed 885 VCs at 681 firms—the most comprehensive empirical study of VC evaluation criteria. Published in Journal of Financial Economics, it won the JFE Best Paper Award and First Prize Jensen Prize 2020.

**Key findings on evaluation weighting:**
- Management team mentioned as important by **95% of VCs**, as MOST important by **47%**
- Business model: 83% important
- Product: 74% important  
- Market: 68% important
- Business-related factors ranked as most important by only 37%

**Critical insight:** Selection is paramount—deal selection rated most important by 49% of VCs versus post-investment value-add (27%) versus deal flow (23%).

**Early-stage valuation methods:** 9% overall (17% early-stage) use NO quantitative metric. 20% overall (31% early-stage) don't forecast cash flows. Most common methods: cash-on-cash multiples and IRR. DCF/NPV rarely used.

### DocSend pitch deck data reveals investor attention patterns

DocSend's ongoing analysis of thousands of pitch decks shows:
- Average VC time on pitch deck: **2 min 36 sec to 3 min 44 sec** (record lows 2021-2023)
- Time spent decreased 12% from 2020 to 2021
- **Sections receiving increased investor attention:** Competitive landscape, product readiness, business model
- 28% of investors prioritize market opportunity
- "Why Now?" slide becoming increasingly important

### Cognitive biases documented in academic literature

Researchers have identified systematic biases affecting investor decisions:
- **Overconfidence bias** - VCs overestimate ability to pick winners
- **Similarity bias** - VCs prefer founders similar to themselves (Franke et al. 2006)
- **Anchoring** - Over-relying on initial information/valuation
- **Herding behavior** - Following other VCs' investment patterns

Zacharakis & Meyer (1998, 2000) found VCs may not be aware of own decision-making processes, and actuarial decision models could improve investment decisions.

---

## Structured rubrics exist but remain underutilized

### The Forge Venture Rubric: 100-point diagnostic scale

The Forge Venture Rubric provides a **100-point scale** with 6 primary parameters and 24 attributes (4 per parameter): Market Opportunity, Technology Advantage, Product/Solution, Team, Business Model, and Growth Potential.

### The Critical Factor Assessment achieves 85% prediction accuracy

The Canadian Innovation Centre's **Critical Factor Assessment (CFA)** has been deployed 20,000+ times with post-decision accuracy studies showing CFA more accurate than investors' own decisions. Recent research (Katcharovski et al., 2025) used AI to replicate CFA with **85% accuracy predicting deal outcomes**.

### Ulu Ventures seed-stage rubric

Published by Clint Korver at Ulu Ventures, this rubric evaluates: Investment thesis fit (Enterprise IT, Diversity, Stanford connection), authentic voice and meaningful purpose, and ethics/intellectual honesty.

---

## Synthesis: universal evaluation dimensions and stage-specific thresholds

### Cross-framework evaluation dimensions ranked by importance

| Dimension | Ranking | Evidence |
|-----------|---------|----------|
| **Team/Founder Quality** | #1 consistently | Gompers (95% cite), Muzyka (5 of top 7 criteria), Techstars (50%+ weight), YC (primary criterion) |
| **Market Size/Opportunity** | #2-3 | DocSend (28% prioritize), Gompers (68%), Sequoia (core criterion) |
| **Business Model** | #2-4 | Gompers (83%), DocSend (increased attention) |
| **Product/Technology** | #3-4 | Gompers (74%), Kaplan & Strömberg (2004) |
| **Traction/Customer Adoption** | #4-5 | Context-dependent; less important for pre-product |
| **Timing/"Why Now"** | Emerging priority | Sequoia (emphasized), DocSend (increasing importance) |

### Stage-specific thresholds for pre-product/pre-revenue ventures

**Pre-product (Level 1/Nascent):**
- 3-5 customers via warm introductions sufficient
- 1/10 to 1/20 warm intro conversion acceptable
- No renewals expected
- Gross margin/burn multiple metrics not applicable
- Primary evaluation: Founder quality, insight into problem, clarity of communication

**Pre-revenue with MVP (Level 2/Developing):**
- 10% first call to closed-won rate target
- Magic Number 0.5-0.75 acceptable
- NRR ≥100%, churn 10-20% acceptable
- Gross margin ≥50% expected
- Burn multiple ≤5X (10X+ problematic)

### Universal knockout criteria across evaluator types

**Immediate red flags:**
- Inability to articulate company purpose in single sentence
- No clear "why now" timing rationale
- Customers who like but don't need the product
- Burn multiple >5X without clear efficiency path
- Founders not coachable or adaptable
- No differentiation from competition
- Design partners hesitant to convert to paid

**Administrative knockouts (federal grants):**
- Small business eligibility failure
- PI employment requirements not met
- Missing certifications or registrations
- Format non-compliance

---

## Conclusion: actionable framework hierarchy for evaluation skill development

The research reveals a clear hierarchy of framework utility for evaluating pre-product and pre-revenue ventures:

**Tier 1 (Most Actionable with Explicit Scoring):**
- NIH 1-9 scoring scale with five criteria—adaptable to non-grant contexts
- First Round Capital's PMF Method—most structured VC framework with quantitative benchmarks
- MassChallenge judging rubric—transparent multi-round process with defined criteria

**Tier 2 (Structured but Without Numerical Scores):**
- Sequoia's 10-slide framework with two core criteria
- Techstars 4-attribute team evaluation
- Third Derivative climate impact thresholds
- DOE three-criteria equally-weighted system

**Tier 3 (Heuristic-Based with Published Philosophy):**
- a16z "magnitude of strength" approach
- Y Combinator founders-first philosophy
- Gompers study empirical weightings

The most significant gap in published frameworks is the lack of explicit quality-level indicators (poor/fair/good/excellent) beyond NIH's scoring descriptors. First Round's level-specific benchmarks and red flags partially address this for SaaS metrics, but equivalent frameworks for other business models remain unpublished or proprietary.

**Frameworks referenced but not publicly accessible:** ARCH Venture Partners detailed criteria, Flagship Pioneering "origination" methodology, Ribbit Capital internal frameworks, a16z internal scoring systems. These are noted as "Unverified - requires confirmation" if cited elsewhere.